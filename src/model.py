import torch
import torch.nn as nn
# from config import MODEL

# ============================
# ğŸ§  LSTM ê¸°ë°˜ Fault ë¶„ë¥˜ ëª¨ë¸
# ============================

# ============================
# 0623, ì •ìƒì ìœ¼ë¡œ ì‘ë™í–ˆë˜ ì½”ë“œ. config.pyë¥¼ ìˆ˜ì •í•˜ë©´ì„œ í˜¹ì‹œ ëª°ë¼ ì£¼ì„ ì²˜ë¦¬..
# ============================
class LSTMClassifier(nn.Module):
    def __init__(self,
                 input_size=3,         #  ì…ë ¥ ì°¨ì›: ia_n, ib_n, ic_n (ê¸°ë³¸ = 3)
                 hidden_size=32,       #  LSTM ì…€ì˜ hidden ìƒíƒœ í¬ê¸°
                 num_layers=2,         #  LSTM ë ˆì´ì–´ ê°œìˆ˜
                 output_size=2,        #  ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ (ì •ìƒ/ê³ ì¥ = 2)
                 dropout=0.3):         #  Dropout ë¹„ìœ¨ (ì¸µ ì‚¬ì´ì— ì ìš©)

        super(LSTMClassifier, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM Layer êµ¬ì„±
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0.0,  # 1ì¸µì¼ ë•ŒëŠ” dropout X
            batch_first=True  # (batch, seq_len, input_size) í˜•íƒœ ì…ë ¥
        )

        # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden stateë§Œ ë°›ì•„ì„œ ë¶„ë¥˜ê¸°ë¡œ ì—°ê²°
        self.fc = nn.Linear(hidden_size, output_size)  # (batch, output_size)

    def forward(self, x):
        """
        x: shape (batch_size, seq_len, input_size)
        output: shape (batch_size, output_size)
        """
        # LSTM í†µê³¼
        lstm_out, (hn, cn) = self.lstm(x)  # lstm_out: (batch, seq_len, hidden)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© (lstm_out[:, -1, :])
        final_hidden = lstm_out[:, -1, :]  # shape: (batch, hidden)

        # FC ë ˆì´ì–´ í†µê³¼ (ë¶„ë¥˜ê¸°)
        out = self.fc(final_hidden)       # shape: (batch, output_size)

        return out


# # ============================
# # ğŸ§  LSTM ê¸°ë°˜ Fault ë¶„ë¥˜ ëª¨ë¸
# # ============================

# class LSTMClassifier(nn.Module):
#     def __init__(self,
#                  input_size=MODEL["input_size"],
#                  hidden_size=MODEL["hidden_size"],
#                  num_layers=MODEL["num_layers"],
#                  output_size=MODEL["output_size"],
#                  dropout=MODEL["dropout"]):

#         super(LSTMClassifier, self).__init__()

#         self.hidden_size = hidden_size
#         self.num_layers = num_layers

#         # LSTM Layer êµ¬ì„±
#         self.lstm = nn.LSTM(
#             input_size=input_size,
#             hidden_size=hidden_size,
#             num_layers=num_layers,
#             dropout=dropout if num_layers > 1 else 0.0,
#             batch_first=True  # (batch, seq_len, input_size)
#         )

#         # ë§ˆì§€ë§‰ ì‹œì ì˜ hidden stateë§Œ ë°›ì•„ì„œ ë¶„ë¥˜ê¸°ë¡œ ì—°ê²°
#         self.fc = nn.Linear(hidden_size, output_size)  # (batch, output_size)

#     def forward(self, x):
#         """
#         x: shape (batch_size, seq_len, input_size)
#         output: shape (batch_size, output_size)
#         """
#         # LSTM í†µê³¼
#         lstm_out, _ = self.lstm(x)  # lstm_out: (batch, seq_len, hidden)

#         # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©
#         final_hidden = lstm_out[:, -1, :]  # (batch, hidden)

#         # FC ë ˆì´ì–´ í†µê³¼
#         out = self.fc(final_hidden)       # (batch, output_size)

#         return out
